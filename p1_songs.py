# -*- coding: utf-8 -*-
"""p1_songs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NzvQXKQdCXda5qtpHIFoCWjYiCE1rfnW

# Regression in the Year Prediction
"""

# Download the data csv file from https://drive.google.com/file/d/1QLvpwOV9VJL98BBSYSe1TYUuIxw7i9kH
# Read about the dataset here https://samyzaf.com/ML/song_year/song_year.html
# (we will use a reduced version of the dataset)

# Upload the file using colab upload functions
# from google.colab import files
# from google.colab import drive
# drive.mount('/content/drive')

# Definir el directorio donde están las imágenes y los textos
folder = '/content/drive/MyDrive/Informatica/Master/Machine_Learning/Practicas/'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# Load the data from the file
data = pd.read_csv('YearPredictionMSD.csv')

# data = pd.read_csv(folder + 'YearPredictionMSD.csv')
X = data.to_numpy()[:,1:] #Características
y = data.to_numpy()[:,0] #Años

from sklearn.preprocessing import StandardScaler
# Escalar los datos
scaler = StandardScaler()
X = scaler.fit_transform(X)

"""## 1.1 Split the data"""

from sklearn.model_selection import train_test_split

# Dividir en entrenamiento y test (80% entrenamiento, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Segundo paso: dividir el conjunto de entrenamiento en entrenamiento y validación (90% de entrenamiento, 10% de validación)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42)  # 0.125 * 0.8 = 0.1 del total

"""## 1.2 Explore the data"""

num_samples, num_features = X.shape

# Reporte del tamaño del dataset
print(f"Num muestras: {num_samples}")
print(f"Num características (dimensiones): {num_features}")

import matplotlib.pyplot as plt
import seaborn as sns

# Distribución del año (variable objetivo)
plt.figure(figsize=(10, 6))
sns.histplot(y, bins=30, kde=True)
plt.title("Distribución del Año (variable objetivo)")
plt.show()

X = data.iloc[:, 1:]
y = data.iloc[:, 0]

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled_df = pd.DataFrame(X_scaled)

# Calcular la matriz de correlación entre las características
corr_matrix = X_scaled_df.corr()

# Graficar la matriz de correlación
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, cmap='coolwarm', center=0)
plt.title("Matriz de Correlación de las Características")
plt.show()

"""Debido a que se puede observar claramente que hay características que están muy correlacionadas entre sí,se ha considerado reducir la cantidad de características usando técnicas como el PCA"""

varianzas = X_scaled.var(axis=0)
caracteristica_mayor_varianza = varianzas.argmax()
print(f"Característica con mayor varianza: {caracteristica_mayor_varianza} con varianza de {varianzas[caracteristica_mayor_varianza]}")

# Calcular la matriz de correlación
corr_matrix = pd.DataFrame(X_scaled).corr()

# Extraer las correlaciones más altas (excluyendo la diagonal)
corr_unstacked = corr_matrix.unstack()
corr_unstacked = corr_unstacked[corr_unstacked != 1]  # Excluir correlaciones perfectas de una característica consigo misma

# Encontrar el par de características con la correlación positiva más alta
max_corr = corr_unstacked.idxmax()
print(f"Par de características con mayor correlación positiva: {max_corr} con valor de {corr_unstacked[max_corr]}")

# Encontrar el par de características con la correlación negativa más alta
min_corr = corr_unstacked.idxmin()
print(f"Par de características con mayor correlación negativa: {min_corr} con valor de {corr_unstacked[min_corr]}")

# Identificar características con poca correlación (cercanas a 0) con todas las demás
low_corr_features = (corr_matrix.abs().mean() < 0.1).index
low_corr_features = [i for i in low_corr_features if corr_matrix.abs().mean()[i] < 0.1]
print(f"Características con poca correlación con las demás: {low_corr_features}")

"""## 1.3 Train several promising models"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

def plot_regression(y_val, y_pred):
  # Crear el gráfico de dispersión de las predicciones vs los valores reales
  plt.figure(figsize=(8, 6))
  plt.scatter(y_val, y_pred, alpha=0.5)
  plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], color='red', lw=2)  # Línea de identidad (predicción perfecta)
  plt.xlabel('Valores Reales')
  plt.ylabel('Predicciones')
  plt.title('Predicciones vs Valores Reales')
  plt.grid(True)
  plt.show()

def plot_error_pred(y_val, y_pred):
  # Calcular los errores de predicción
  errors = y_val - y_pred

  # Graficar el histograma de los errores
  plt.figure(figsize=(8, 6))
  plt.hist(errors, bins=30, edgecolor='black', alpha=0.7)
  plt.xlabel('Error de Predicción (Real - Predicho)')
  plt.ylabel('Frecuencia')
  plt.title('Distribución de los Errores de Predicción')
  plt.grid(True)
  plt.show()


def train(model, X_train, y_train, X_val, y_val):
  model.fit(X_train, y_train)
  y_pred = model.predict(X_val)
  mse = mean_squared_error(y_val, y_pred)
  r2 = r2_score(y_val, y_pred)

  print(f"Error Cuadrático Medio (MSE): {mse}")
  print(f"Coeficiente de Determinación (R²): {r2}")
  plot_regression(y_val, y_pred)

model = LinearRegression()
train(model, X_train, y_train, X_val, y_val)

from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2)
X_poly_train = poly.fit_transform(X_train)
X_poly_val = poly.transform(X_val)

model = LinearRegression()
train(model, X_poly_train, y_train, X_poly_val, y_val)

from sklearn.linear_model import Ridge

# Entrenar un modelo Ridge con regularización
ridge_model = Ridge(alpha=350.0)
train(ridge_model, X_train, y_train, X_val, y_val)

from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
poly = PolynomialFeatures(degree=2)
X_poly_train = poly.fit_transform(X_train)
X_poly_val = poly.transform(X_val)

ridge_model = Ridge(alpha=1000)
train(ridge_model, X_poly_train, y_train, X_poly_val, y_val)

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
train(rf_model, X_train, y_train, X_val, y_val)

from sklearn.linear_model import HuberRegressor

huber_model = HuberRegressor()
train(huber_model, X_train, y_train, X_val, y_val)

from sklearn.linear_model import RANSACRegressor

# Entrenar un modelo de regresión robusta con RANSAC
ransac_model = RANSACRegressor()
train(ransac_model, X_train, y_train, X_val, y_val)

from sklearn.linear_model import Lasso
model = Lasso(alpha=0.005)
train(model, X_train, y_train, X_val, y_val)

from sklearn.linear_model import SGDRegressor

# Entrenar un modelo de regresión con Descenso de Gradiente Estocástico
sgd_model = SGDRegressor(max_iter=2000, tol=1e-3)
train(sgd_model, X_train, y_train, X_val, y_val)

from sklearn.tree import DecisionTreeRegressor

tree_model = DecisionTreeRegressor()
train(tree_model, X_train, y_train, X_val, y_val)

param_grid = {'alpha': [0.1, 1.0, 10.0, 100.0]}
grid_search = GridSearchCV(Ridge(), param_grid, scoring='neg_mean_squared_error', cv=5)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
train(best_model, X_train, y_train, X_val, y_val)

"""## Test

Hemos elegido el modelo el modelo con menos RSM:
"""

model = LinearRegression()