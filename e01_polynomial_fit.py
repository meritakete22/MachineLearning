# -*- coding: utf-8 -*-
"""e01_polynomial_fit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lFXaxHFj3rldClsAEFf16ACdTU_b2QP2
"""

import numpy as np
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from google.colab import files
from matplotlib import pyplot as plt
from matplotlib import rcParams
rcParams['figure.figsize'] = (10, 6)
rcParams['legend.fontsize'] = 16
rcParams['axes.labelsize'] = 16

# Upload the file
uploaded = files.upload()

# Load the data from the file
X, y = np.load('e01_data.npy')

# Make sure the data is there
print(X.shape)
print(y.shape)
plt.scatter(X, y, color='yellowgreen', marker='.',label='Data')
plt.legend(loc='lower right')

# Do your machine learning here
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error

# SMALL NOTE: To expand the input vector to polynomial features, e.g. of degree 2, do:
poly_10 = PolynomialFeatures(10)
poly_5 = PolynomialFeatures(5)
poly_2 = PolynomialFeatures(2)
y_disc = (y > np.median(y)).astype(int)


X_poly_10 = poly_10.fit_transform(X)
X_poly_5 = poly_5.fit_transform(X)
X_poly_2 = poly_2.fit_transform(X)
print(X_poly_2.shape)
#print(X_poly_2)
print(X_poly_5.shape)
#print(X_poly_5)
print(X_poly_10.shape)
#print(X_poly_10)

# Fit the polynomial model with Linear Regression
model_2 = LinearRegression()
model_5 = LinearRegression()
model_10 = LinearRegression()
model_2.fit(X_poly_2, y)
model_5.fit(X_poly_5, y)
model_10.fit(X_poly_10, y)
# RegresiÃ³n Ridge
model_ridge_5 = Ridge(alpha=1.0)
model_ridge_5.fit(X_poly_5, y)

# Logistic Regression
model_log = LogisticRegression()
model_log.fit(X_poly_5, y_disc)


# Print the coefficients
#print(model_2.coef_)
#print(model_2.intercept_)

print("SCORES")
print("Grade 2:", model_2.score(X_poly_2, y))
print("Grade 5:", model_5.score(X_poly_5, y))
print("Grade 10:", model_10.score(X_poly_10, y))
print("Logistic Regression:", model_log.score(X_poly_5, y_disc))
print("Ridge Grade 5:", model_ridge_5.score(X_poly_5, y))

# Generar un rango de valores para X
X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)

# Generar un rango de valores para X
X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
X_range_poly_2 = poly_2.transform(X_range)
X_range_poly_5 = poly_5.transform(X_range)
X_range_poly_10 = poly_10.transform(X_range)

# Predecir los valores de y
y_pred_2 = model_2.predict(X_range_poly_2)
y_pred_5 = model_5.predict(X_range_poly_5)
# y_pred_log = model_log.predict(X_range_poly_5)
y_pred_log = model_log.predict_proba(X_range_poly_5)[:, 1]
y_pred_10 = model_10.predict(X_range_poly_10)
y_pred_ridge_5 = model_ridge_5.predict(X_range_poly_5)

# Calculate MSE for predictions without cross-validation
mse_2 = mean_squared_error(y, model_2.predict(X_poly_2))
mse_5 = mean_squared_error(y, model_5.predict(X_poly_5))
mse_10 = mean_squared_error(y, model_10.predict(X_poly_10))
mse_ridge_5 = mean_squared_error(y, model_ridge_5.predict(X_poly_5))

# Calculate MSE for predictions with cross-validation
mse_2_cv = mean_squared_error(y, y_pred_2_cv)
mse_5_cv = mean_squared_error(y, y_pred_5_cv)
mse_10_cv = mean_squared_error(y, y_pred_10_cv)
mse_ridge_5_cv = mean_squared_error(y, y_pred_ridge_5_cv)
mse_log_cv = mean_squared_error((y > np.median(y)).astype(int), y_pred_log_cv)

# Print MSE values
print("\nMSE Without Cross-Validation:")
print(f"Linear Regression Grade 2: {mse_2:.4f}")
print(f"Linear Regression Grade 5: {mse_5:.4f}")
print(f"Linear Regression Grade 10: {mse_10:.4f}")
print(f"Ridge Regression Grade 5: {mse_ridge_5:.4f}")

print("\nMSE With Cross-Validation:")
print(f"Linear Regression Grade 2: {mse_2_cv:.4f}")
print(f"Linear Regression Grade 5: {mse_5_cv:.4f}")
print(f"Linear Regression Grade 10: {mse_10_cv:.4f}")
print(f"Ridge Regression Grade 5: {mse_ridge_5_cv:.4f}")
print(f"Logistic Regression: {mse_log_cv:.4f}")

from sklearn.model_selection import cross_val_predict
# Now the same as before but using cross validation
y_pred_2_cv = cross_val_predict(model_2, X_poly_2, y, cv=5)
y_pred_5_cv = cross_val_predict(model_5, X_poly_5, y, cv=5)
y_pred_10_cv = cross_val_predict(model_10, X_poly_10, y, cv=5)
y_pred_log_cv = cross_val_predict(model_log, X_poly_5, y_disc, cv=5)
y_pred_ridge_5_cv = cross_val_predict(model_ridge_5, X_poly_5, y, cv=5)

# Plotting
plt.figure(figsize=(12, 6))

# Plot predictions without cross-validation
plt.subplot(1, 2, 1)
plt.scatter(X, y, color='yellowgreen', marker='.', label='Datos originales')
plt.plot(X_range, y_pred_2, color='red', label='Linear Regression Grade 2 (no CV)')
plt.plot(X_range, y_pred_5, color='blue', label='Linear Regression Grade 5 (no CV)')
plt.plot(X_range, y_pred_10, color='green', label='Linear Regression Grade 10 (no CV)')
plt.plot(X_range, y_pred_ridge_5, color='orange', label='Ridge Regression Grade 5 (no CV)')
plt.plot(X_range, y_pred_log, color='black', label='Logistic Regression Grade 5 (no CV)')
plt.xlabel('X')
plt.ylabel('y')
plt.legend(loc='lower left', fontsize='small')
plt.title('Predictions without CV')

# Plot predictions with cross-validation
plt.subplot(1, 2, 2)
plt.scatter(X, y, color='yellowgreen', marker='.', label='Datos originales')
plt.plot(X_range, model_2.predict(X_range_poly_2), color='red', label='Linear Regression Grade 2 (CV)')
plt.plot(X_range, model_5.predict(X_range_poly_5), color='blue', label='Linear Regression Grade 5 (CV)')
plt.plot(X_range, model_10.predict(X_range_poly_10), color='green', label='Linear Regression Grade 10 (CV)')
plt.plot(X_range, model_ridge_5.predict(X_range_poly_5), color='orange', label='Ridge Regression Grade 5 (CV)')
plt.plot(X_range, model_log.predict_proba(X_range_poly_5)[:, 1], color='black', label='Logistic Regression Grade 5 (CV)')

plt.xlabel('X')
plt.ylabel('y')
plt.legend(loc='lower left', fontsize='small')
plt.title('Predictions with CV')

plt.tight_layout()
plt.show()